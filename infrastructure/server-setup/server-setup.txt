# This Source Code Form is subject to the terms of the Mozilla Public
# License, v. 2.0. If a copy of the MPL was not distributed with this
# file, You can obtain one at https://mozilla.org/MPL/2.0/.
#
# OpenCRVS is also distributed under the terms of the Civil Registration
# & Healthcare Disclaimer located at http://opencrvs.org/license.
#
# Copyright (C) The OpenCRVS Authors. OpenCRVS and the OpenCRVS
# graphic logo are (registered/a) trademark(s) of Plan International.
# Manual server setup notes

# add users
curl https://github.com/rcrichton.keys >> ~/.ssh/authorized_keys
curl https://github.com/euanmillar.keys >> ~/.ssh/authorized_keys
curl https://github.com/rikukissa.keys >> ~/.ssh/authorized_keys
curl https://github.com/mushrafulhoque-dsi.keys  >> ~/.ssh/authorized_keys

# prevent password login
sed -i 's/PasswordAuthentication.*/PasswordAuthentication no/' /etc/ssh/sshd_config
sudo systemctl restart ssh

# install docker
curl -fsSL get.docker.com -o get-docker.sh
sh get-docker.sh

# create swarm
# on manger:
docker swarm init --advertise-addr <MANAGER-IP>

# on workers (the command is given to you when the manager inits, but something like):
docker swarm join --token <TOKEN> opencrvs-staging.jembi.org:2377

# check state from manager:
docker node ls
# login to private registry on all nodes
docker login

# on manger, create required directories
mkdir -p /data/mongo
mkdir -p /data/traefik
mkdir -p /data/elasticsearch
chmod g+rwx /data/elasticsearch
chgrp 1000 /data/elasticsearch
echo vm.max_map_count=262144 | sudo tee -a /etc/sysctl.conf && sudo sysctl -p
mkdir -p /data/influxdb
touch /data/traefik/acme.json
chmod 600 /data/traefik/acme.json

# on all nodes
Add this to the crontab to clean up autodeploys ->
  0 0 * * * /usr/bin/docker system prune -f >> /var/log/docker-prune.log

# to deploy, check Docker Hub for the hash associated with the lastest docker images created by Travis.  Then run

yarn deploy:qa <docker-hub-hash>

# After deploy:

mkdir /tmp/compose/administrative
mkdir /tmp/compose/facilities

#You need to check Docker swarm for the id of the containers running mongo, elasticsearch or resources in order to access

#To find which node hosts the container you are looking for

docker stack ps -f "desired-state=running" opencrvs

#SSH into the right node and run the following to get container id

docker ps

#To access mongo:

docker exec -it <mongo-container-id> mongo

#To access bash:

docker exec -it <resources-or-user-mgnt-container-id> /bin/bash

# in resource service container run:

FHIR_URL=http://hearth:3447/fhir yarn populate

# Copy the 2 locations.json files that are generated in the following directory in the resource package to the other running resources container.

docker cp <resources-replica-1-containerId>:/usr/src/app/src/features/administrative/generated/bn/locations.json /tmp/compose/administrative/locations.json
docker cp <resources-replica-1-containerId>:/usr/src/app/src/features/facilities/generated/bn/locations.json /tmp/compose/facilities/locations.json

# exit and copy files to local env

scp root@<remote-ip-of-node-with-replica1>:/tmp/compose/administrative/locations.json /where/to/store/temporarily/administrative/locations.json
scp root@<remote-ip-of-node-with-replica1>:/tmp/compose/facilities/locations.json /where/to/store/temporarily/facilities/locations.json

# copy files to node containing replica 2 of resources package

scp /where/to/store/temporarily/administrative/locations.json root@<remote-ip-of-node-with-replica2>:/tmp/compose/administrative/locations.json
scp where/to/store/temporarily/facilities/locations.json root@<remote-ip-of-node-with-replica2>:/tmp/compose/facilities/locations.json

# SSH into node with replica 2 and copy into container with replica 2

docker cp /tmp/compose/administrative/locations.json <resources-replica-2-containerId>:/usr/src/app/src/features/administrative/generated/bn/locations.json
docker cp /tmp/compose/facilities/locations.json <resources-replica-2-containerId>:/usr/src/app/src/features/facilities/generated/bn/locations.json

#Clear Elastic Search data by accessing bask of the running elastic search container then run:

curl -XDELETE 'http://localhost:9200/*'

# in user mgnt service container:

yarn populate
